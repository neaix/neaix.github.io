---
layout: post
title: "Python爬虫基础学习"
comments: true
share: true
tags: Python
---

## 一. 概览： ##

HTTP协议大致流程：

一次HTTP操作称为一个事务，其工作过程可分为四步：

- 1）首先客户机与服务器需要建立连接。只要单击某个超级链接，HTTP的工作开始。
- 2）建立连接后，客户机发送一个请求给服务器，请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是MIME信息包括请求修饰符、客户机信息和可能的内容。
- 3）服务器接到请求后，给予相应的响应信息，其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是MIME信息包括服务器信息、实体信息和可能的内容。
- 4）客户端接收服务器所返回的信息通过浏览器显示在用户的显示屏上，然后客户机与服务器断开连接。

如果在以上过程中的某一步出现错误，那么产生错误的信息将返回到客户端，有显示屏输出。

请求报头允许客户端向服务器端传递请求的附加信息以及客户端自身的信息。

常用的请求报头

- Accept

	Accept请求报头域用于指定客户端接受哪些类型的信息。eg：Accept：image/gif，表明客户端希望接受GIF图象格式的资源；Accept：text/html，表明客户端希望接受html文本。

- Accept-Charset

	Accept-Charset请求报头域用于指定客户端接受的字符集。eg：Accept-Charset:iso-8859-1,gb2312.如果在请求消息中没有设置这个域，缺省是任何字符集都可以接受。

- Accept-Encoding

	Accept-Encoding请求报头域类似于Accept，但是它是用于指定可接受的内容编码。eg：Accept-Encoding:gzip.deflate.如果请求消息中没有设置这个域服务器假定客户端对各种内容编码都可以接受。

- Accept-Language

	Accept-Language请求报头域类似于Accept，但是它是用于指定一种自然语言。eg：Accept-Language:zh-cn.如果请求消息中没有设置这个报头域，服务器假定客户端对各种语言都可以接受。

- Authorization

	Authorization请求报头域主要用于证明客户端有权查看某个资源。当浏览器访问一个页面时，如果收到服务器的响应代码为401（未授权），可以发送一个包含Authorization请求报头域的请求，要求服务器对其进行验证。

- Host（**发送请求时，该报头域是必需的**）

	Host请求报头域主要用于指定被请求资源的Internet主机和端口号，它通常从HTTP URL中提取出来的，eg：

	我们在浏览器中输入：http://www.guet.edu.cn/index.html

	浏览器发送的请求消息中，就会包含Host请求报头域，如下：

	Host：www.guet.edu.cn

	此处使用缺省端口号80，若指定了端口号，则变成：Host：www.guet.edu.cn:指定端口号

- User-Agent

	User-Agent请求报头域允许客户端将它的操作系统、浏览器和其它属性告诉服务器。不过，这个报头域不是必需的，如果我们自己编写一个浏览器，不使用User-Agent请求报头域，那么服务器端就无法得知我们的信息了。

- 请求报头举例：

<code>
GET /form.html HTTP/1.1 (CRLF)

Accept:image/gif,image/x-xbitmap,image/jpeg,application/x-shockwave-flash,application/vnd.ms-excel,application/vnd.ms-powerpoint,application/msword,*/* (CRLF)

Accept-Language:zh-cn (CRLF)

Accept-Encoding:gzip,deflate (CRLF)

If-Modified-Since:Wed,05 Jan 2007 11:21:25 GMT (CRLF)

If-None-Match:W/"80b1a4c018f3c41:8317" (CRLF)

User-Agent:Mozilla/4.0(compatible;MSIE6.0;Windows NT 5.0) (CRLF)

Host:www.guet.edu.cn (CRLF)

Connection:Keep-Alive (CRLF)
</code>


####  爬虫主要结构：  ####

	'''
	#伪代码:
	#所有爬虫的主要结构:

	import Queue #队列
	initial_page = "http://www.renminribao.com"
	
	url_queue = Queue.Queue() #爬取队列
	
	seen = set() #已经爬过了的
	seen.insert(initial_page)
	
	url_queue.put(initial_page)
	
	while(True): #一直进行直到海枯石烂
	    if url_queue.size()>0:
	        current_url = url_queue.get()    #拿出队例中第一个的url
	        store(current_url)               #把这个url代表的网页存储好
	        for next_url in extract_urls(current_url): #提取这个url里链向的url
	            if next_url not in seen:      
	                seen.put(next_url)
	                #在这里可以对next_url进行解析操作.
	                url_queue.put(next_url) #放进爬取队列
	    else:
	        break
	
	
	-----------------------------
	
	上述代码只是一个结构,实际使用起来效率不高.
	
	因为所有网页要遍历一次而每次判重用set的话需要log(N)的复杂度
	
	通常的判重做法是使用Bloom Filter. 
	
	它仍然是一种hash的方法,但是它的特点是它可以使用固定的内存,不随url的数量而增长,
	以O(1)的效率判定url是否已经在set中。
	
	它的唯一问题在于,如果这个url不在set中,BF可以100%确定这个url没有看过。
	但是如果这个url在set中,它会告诉你,这个url应该已经出现过,不过我有2%的不确定性。
	这里的不确定性在分配的内存足够大的时候可以变得很小很少。
	
	另一个瓶颈是:一台机器数量太少(这里还要用多进程使每台机器效率充分利用).
	所以就要用到集群化爬取.
	
	把这100台中的99台运算能力较小的机器叫作slave,一台较大的机器叫作master.
	把上面代码中的url_queue放到这台master机器上,
	而所有的slave都可以通过网络跟master联通.
	每当一个slave完成下载一个网页就向master请求一个新的网页来抓取。
	而每次slave新抓到一个网页就把这个网页上所有的链接送到master的url_queue里去。
	同样bloom filter也放到master上.
	但是现在master只发送确定没有被访问过的url给slave。
	Bloom Filter放到master的内存里而被访问过的url放到运行在master上的Redis里这样保证所有操作都是O(1)。
	至少平摊是O(1)
	
	
	考虑如何用python实现
	
	在各台slave上装好scrapy那么各台机子就变成了一台有抓取能力的slave
	在master上装好Redis和rq用作分布式队列。
	
	'''
	
	#代码于是写成:
	
	
	#slave.py
	current_url = request_from_master() #函数,从master处获取url
	to_send = [] #要发送过去的url列表
	for next_url in extract_urls(current_url):
	    to_send.append(next_url)
	
	store(current_url); #保存url内容
	#在这里可以对current进行解析
	send_to_master(to_send) #发送给master
	
	#master.py
	distributed_queue = DistributedQueue() #分布式队列
	bf = BloomFilter() #用来保存已经浏览过的url
	
	initial_pages = "www.renmingribao.com"
	
	while(True):#不进行处理url内容,master只负责分发和管理url池
	    if request == 'GET':#slave请求得到url
	        if distributed_queue.size()>0:
	            send(distributed_queue.get())
	        else:
	            break
	    elif request == 'POST':#slave发送过来一个url
	        bf.put(request.url)
	        
	#完整的分布式爬虫解决方案:
	#https://github.com/darkrho/scrapy-redis


---

## 二. urllib库的使用： ##

####  1.最简单的抓取并读出：  ####

	import urllib.request
	 
	url = "https://joway.github.io"
	
	#urlopen()返回一个 http.client.HTTPResponse 对象
	data = urllib.request.urlopen(url).read()
	data = data.decode('UTF-8')#对百度和豆瓣操作的时候这个加了会报错
	print(data)


####  2.构造url并伪装成浏览器进行爬取：  ####

	import urllib
	import urllib.request
	
	data={} #字典
	data['word']='Joway'
	
	#urlencode()把key-value这对键值转换成我们url里所需要的形式:
	'''eg:
	data = {'a': 'test','name': '魔兽' }
	print(urllib.parse.urlencode(data)) 
	得到的结果为:'a=test&amp;name=%C4%A7%CA%DE'
	附带了把中文转码的功能
	'''
	url_values=urllib.parse.urlencode(data)
	
	url="http://www.baidu.com/s?"
	full_url=url+url_values#构造完整url
	
	data=urllib.request.urlopen(full_url).read()
	#data=data.decode('UTF-8')
	print(data)


这个爬虫链接发送出去的报文如下：

	GET /s?word=Joway HTTP/1.1
	Accept-Encoding: identity
	Host: www.baidu.com
	User-Agent: Python-urllib/3.4
	Connection: close

而我用浏览器访问产生的报文是：

	GET / HTTP/1.1
	Host: www.baidu.com
	Connection: keep-alive
	Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
	User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2438.3 Safari/537.36
	HTTPS: 1
	Accept-Encoding: gzip, deflate, sdch
	Accept-Language: zh-CN,zh;q=0.8
	Cookie: BAIDUID=A6D1EE5F54BE7EFB8B101B2525F877A3:FG=1; BDUSS=lVGZFRHVjlzamZSWnRya2pkWDhiQ2xNbDJ6azVDV1h6UGxEOFZ0U1IyR2RPZVZWQVFBQUFBJCQAAAAAAAAAAAEAAAC1DNYYSm93YXlfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJ2svVWdrL1VQ; PSTM=1438848560; BIDUPSID=63E2477F041A4B1714273178665B0594; locale=zh; cflag=65151%3A3; H_PS_645EC=5d50HZaUnx%2BzTOSQXPT01J7%2FHPWXBnTsvvXhrKAoEOpkwSgjDkdEmiih%2FNIkEn7fYsr9; BDRCVFR[feWj1Vr5u3D]=mk3SLVN4HKm; BD_CK_SAM=1; BD_HOME=1; H_PS_PSSID=16229_1446_16779_12772_12825_14431_12868_16520_16800_14952_16424_16514_15638_12330_13932_16721_10632_16866; BD_UPN=12314753


	
所以上面只是简单的抓取了网站而已，并没有真正模仿浏览器。

下面样例可以手动构造请求报文：

	import urllib.request
	
	weburl = "http://www.douban.com/"
	webheader = {
	    'Connection': 'Keep-Alive',
	    'Accept': 'text/html, application/xhtml+xml, */*',
	    'Accept-Language': 'en-US,en;q=0.8,zh-Hans-CN;q=0.5,zh-Hans;q=0.3',
	    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',
	    #'Accept-Encoding': 'gzip, deflate',
	    'Host': 'www.douban.com',
	    'DNT': '1'
	    }
	
	req = urllib.request.Request(url=weburl, headers=webheader)  
	
	webPage=urllib.request.urlopen(req)
	data = webPage.read()
	#data = data.decode('UTF-8')
	print(data)
	print(type(webPage))
	print(webPage.geturl())
	print(webPage.info())
	print(webPage.getcode())

得到的报文为：

	GET / HTTP/1.1
	Accept-Encoding: identity
	Connection: close
	Host: www.douban.com
	Accept: text/html, application/xhtml+xml, */*
	User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko
	Accept-Language: en-US,en;q=0.8,zh-Hans-CN;q=0.5,zh-Hans;q=0.3
	Dnt: 1

除了利用urllib.request.Request()构造报文外，还可以用更加强大灵活的build_opener(),这个方法还能够处理Cookies 

	import urllib.request
	import http.cookiejar
	
	# 注意下面是一个很长的函数,用来构造报文头
	# 返回值是一个已经加工好了的urllib.request.build_opener()返回值
	def makeMyOpener(head = {
	    'Connection': 'Keep-Alive',
	    'Accept': 'text/html, application/xhtml+xml, */*',
	    'Accept-Language': 'en-US,en;q=0.8,zh-Hans-CN;q=0.5,zh-Hans;q=0.3',
	    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko'
	}):
	    cj = http.cookiejar.CookieJar() #cookie压缩包
	    opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))
	    header = []
	    for key, value in head.items():
	        elem = (key, value)
	        header.append(elem)
	    opener.addheaders = header
	    return opener
	
	oper = makeMyOpener()
	uop = oper.open('http://www.baidu.com/', timeout = 1000)
	data = uop.read()
	print(data)



####  3.基于广度优先搜索(BFS)算法并能自动跳转和保存本地文件的爬虫  ####

BFS的实现以来于数据结构队列。

Python的List功能已经足够完成队列的功能, 可以用 append() 来向队尾添加元素, 可以用 pop(0) 来弹出队首元素. 但是List用来完成队列功能其实是低效率的, 因为List在队首使用 pop(0) 和 insert() 都是效率比较低的, Python官方建议使用collection.deque来高效的完成队列任务.

collection.deque例子：

	from collections import deque
	queue = deque(["Eric", "John", "Michael"])
	queue.append("Terry")           # Terry 入队
	queue.append("Graham")          # Graham 入队
	queue.popleft()                 # 队首元素出队
	#输出: 'Eric'
	queue.popleft()                 # 队首元素出队
	#输出: 'John'
	queue                           # 队列中剩下的元素
	#输出: deque(['Michael', 'Terry', 'Graham'])

另外，我们需要对未爬取过的url放入集合set中。(这里先不考虑set的效率。如概览中可见，为了高效可以用BloomFilter代替集合)

set是一种无序的, 不包含重复元素的结构. 

下面是一个简单示例：

	import re
	import urllib.request
	import urllib
	
	from collections import deque
	
	queue = deque()
	visited = set()
	
	url = 'http://news.dbanotes.net'  # 入口页面
	
	queue.append(url)
	cnt = 0
	
	while queue:
	  url = queue.popleft()  # 队首元素出队
	  visited |= {url}  # 标记为已访问,相当于把url并进visited
	
	  print('已经抓取: ' + str(cnt) + '   正在抓取 <---  ' + url)
	  cnt += 1
	  urlop = urllib.request.urlopen(url)#当前正在操作的url
	  if 'html' not in urlop.getheader('Content-Type'):#用getheader()函数判断是否是静态的html页面
	    continue
	
	  # 避免程序异常中止, 用try..catch处理异常
	  try:
	    data = urlop.read().decode('utf-8')
	  except:
	    continue
	
	  # 正则表达式提取页面中所有队列, 并判断是否已经访问过, 然后加入待爬队列
	  #网页源代码中所有链接格式都为:href="http://xxx.xxx/.../xxx.xxx"
	  #这里用'href="(.+?)"'匹配到所有href=后的url.
	  #当然,href=后的也并不一定就是url,还有很多杂乱的如news这类无效链接.
	  linkre = re.compile('href="(.+?)"')
	  for x in linkre.findall(data):
	    if 'http' in x and x not in visited:
	      queue.append(x)
	      print('加入队列 --->  ' + x)

这一个版本的爬虫有很多缺点。例如：

- 一个网站上不了, 爬虫却一直在等待连接返回response, 不知道超时跳过; 
- 抓取的内容没有保存到本地, 没有什么作用. 

首先, 将
	
	urlop = urllib.request.urlopen(url)

改为
	try:
		urlop = urllib.request.urlopen(url, timeout = 2)
	except:
		continue

不加异常处理会发现一旦超时会出现exception中断

在爬 http://baidu.com 的时候, 爬回来一个没有什么内容的东西, 这个东西告诉我们应该跳转到 http://www.baidu.com . 但是上一个爬虫并不支持自动跳转, 

首先我们要知道爬 http://baidu.com 的时候会返回的页面:

	<html>
	<meta http-equiv="refresh" content="0;url=http://www.baidu.com/">
	</html>

其中的0是等待0秒后跳转, 也就是立即跳转. 这样再像上一次说的那样用一个正则表达式把这个url提取出来就可以爬到正确的地方去了. 

接下来保存抓回来的报文

顺便说说文件操作. Python 的文件操作还是相当方便的. 我们可以讲抓回来的数据 data 以二进制形式保存, 也可以经过 decode() 处理成为字符串后以文本形式保存. 改动一下打开文件的方式就能用不同的姿势保存文件了. 下面是参考代码:

	def saveFile(data):
	    save_path = 'D:\\temp.out'
	    f_obj = open(save_path, 'wb') # wb 表示打开方式
	    f_obj.write(data)
	    f_obj.close()
 
	# 这里省略爬虫代码
	# ...
 
	# 爬到的数据放到 dat 变量里
	# 将 dat 变量保存到 D 盘下
	saveFile(dat)
 

####  4.具备登录行为的爬虫:  ####

以登录知乎为例:

给 http://www.zhihu.com/login 这个网址发送了一个POST, 内容如下:

	POST http://www.zhihu.com/login HTTP/1.1
	Content-Type: application/x-www-form-urlencoded; charset=UTF-8
	Accept: */*
	X-Requested-With: XMLHttpRequest
	Referer: http://www.zhihu.com/#signin
	Accept-Language: en-US,en;q=0.8,zh-Hans-CN;q=0.5,zh-Hans;q=0.3
	Accept-Encoding: gzip, deflate
	User-Agent: Mozilla/5.0 (Windows NT 6.4; WOW64; Trident/7.0; rv:11.0) like Gecko
	Content-Length: 97
	DNT: 1
	Host: www.zhihu.com
	Connection: Keep-Alive
	Pragma: no-cache
	Cookie: __utma=51854390.1539896551.1412320246.1412320246.1412320246.1; __utmb=51854390.6.10.1412320246; __utmc=51854390; __utmz=51854390.1412320246.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); __utmv=51854390.000--|3=entry_date=20141003=1
	
	_xsrf=4b41f6c7a9668187ccd8a610065b9718&email=xxx%40gmail.com&password=xxxxxxxxx&rememberme=y


注意最后一条:
	
	_xsrf=4b41f6c7a9668187ccd8a610065b9718&email=xxx%40gmail.com&password=xxxxxxxxx&rememberme=y

有用户名, 有密码, 有一个"记住我"的 yes. 这里最开头出现了一个 Name 为 _xsrf 的项, 他的值是 4b41f6c7a9668187ccd8a610065b9718. 我们要先获取这个值, 然后才能给他发.

这个值浏览器是如何获取的呢, 我们刚刚是先访问了 http://www.zhihu.com/ 这个网址, 就是首页, 然后登录的时候他却给 http://www.zhihu.com/login 这个网址发信息. 所以就会发现肯定是首页把 _xsrf 生成发送给我们, 然后我们再把这个 _xsrf 发送给 /login 这个 url. 这样一会儿过后我们就要从第一个 GET 得到的响应报文里面去寻找 _xsrf

当我们在浏览器中登录成功后, 服务器还告诉我们的浏览器如何保存它给出的 Cookies 信息. 所以我们也要用 Python 把这些 Cookies 信息记录下来.

有关Cookies的报文如下:

	Connection: keep-alive
	Server: zhihu_nginx
	Content-Security-Policy: default-src *; frame-src *.zhihu.com getpocket.com note.youdao.com; script-src *.zhihu.com *.google-analytics.com zhstatic.zhihu.com 'unsafe-eval'; style-src *.zhihu.com 'unsafe-inline'
	Set-Cookie: z_c0="QUFDQXZFTWNBQUFYanhhVTQwd1dZdDR3V3dZYjhBPT0=|1439724624|36777710f30e95e36db259d2d70d8956"; Domain=zhihu.com; expires=Mon, 15 Aug 2016 11:30:24 GMT; httponly; Path=/
	Set-Cookie: unlock_ticket="QUFDVmg3MEZWRFR0QWNwR2RRRFhzRWJlalJZZmlGY2wyZUlBPT0=|1439724624|c2969440b511c208d9bf82218db6257e7118810c"; Domain=zhihu.com; expires=Sun, 16 Aug 2015 11:59:24 GMT; Path=/
	Set-Cookie: n_c=; Domain=zhihu.com; expires=Sat, 16 Aug 2014 11:30:24 GMT; Path=/
	Expires: Fri, 02 Jan 2000 00:00:00 GMT
	Vary: Accept-Encoding

接下来开始具体实施.

首先发现知乎Get下来的首页是经过gzip压缩后的数据.需要先解压.

	import gzip
	def ungzip(data):
	    try:        # 尝试解压
	        print('正在解压.....')
	        data = gzip.decompress(data)
	        print('解压完毕!')
	    except:
	        print('未经压缩, 无需解压')
	    return data


通过 opener.read() 读取回来的数据, 经过 ungzip 自动处理后, 再来一遍 decode() 就可以得到解码后的 str 了

如果没有 _xsrf 这个键的值, 或许即便有用户名和密码也无法登录知乎(我没试过, 不过我们学校的教务系统确实如此) 

如上文所说, 在第一遍 GET的时候可以从响应报文中的 HTML 代码里面得到这个 _xsrf. 

如下函数实现了这个功能, 返回的 str 就是 _xsrf 的值.

	import re
	def getXSRF(data):
	    cer = re.compile('name="_xsrf" value="(.*)"', flags = 0)
	    strlist = cer.findall(data)
	    return strlist[0]


对于Cookie的处理:

创建 opener 的时候将一个 HTTPCookieProcessor 放进去, Cookies 的事情就不用我们管了. 

	import http.cookiejar
	import urllib.request
	def getOpener(head):
	    # deal with the Cookies
	    cj = http.cookiejar.CookieJar()
	    pro = urllib.request.HTTPCookieProcessor(cj)
	    opener = urllib.request.build_opener(pro)
	    header = []
	    for key, value in head.items():
	        elem = (key, value)
	        header.append(elem)
	    opener.addheaders = header
	    return opener

getOpener 函数接收一个 head 参数, 这个参数是一个字典. 函数把字典转换成元组集合, 放进 opener. 这样建立的这个 opener 就有两大功能:

- 自动处理使用 opener 过程中遇到的 Cookies
- 自动在发出的 GET 或者 POST 请求中加上自定义的 Header

下面是完整的代码. 不过,现在知乎新增了验证码功能,所以这个代码也失效了. 

	import gzip
	import re
	import http.cookiejar
	import urllib.request
	import urllib.parse
	
	#解压gzip
	def ungzip(data):
	    try:        # 尝试解压
	        print('正在解压.....')
	        data = gzip.decompress(data)
	        print('解压完毕!')
	    except:
	        print('未经压缩, 无需解压')
	    return data
	
	#得到_xsrf键值
	def getXSRF(data):
	    cer = re.compile('name="_xsrf" value="(.*)"', flags = 0)
	    strlist = cer.findall(data)
	    return strlist[0]
	
	#得到opener
	def getOpener(head):
	    # deal with the Cookies
	    cj = http.cookiejar.CookieJar()
	    pro = urllib.request.HTTPCookieProcessor(cj)
	    opener = urllib.request.build_opener(pro)
	    header = []
	    for key, value in head.items():
	        elem = (key, value)
	        header.append(elem)
	    opener.addheaders = header
	    return opener
	
	header = {
	    'Connection': 'Keep-Alive',
	    'Accept': 'text/html, application/xhtml+xml, */*',
	    'Accept-Language': 'en-US,en;q=0.8,zh-Hans-CN;q=0.5,zh-Hans;q=0.3',
	    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',
	    'Accept-Encoding': 'gzip, deflate',
	    'Host': 'www.zhihu.com',
	    'DNT': '1'
	}
	
	url = 'http://www.zhihu.com/'
	opener = getOpener(header)
	op = opener.open(url)
	data = op.read()
	data = ungzip(data)     # 解压
	_xsrf = getXSRF(data.decode())
	
	url += 'login/email'
	id = 'xxx@qq.com'
	password = 'xxxxxxxxx'
	postDict = {
	        '_xsrf':_xsrf,
	        'email': id,
	        'password': password,
	        'rememberme': 'y'
	}
	postData = urllib.parse.urlencode(postDict).encode()
	op = opener.open(url, postData)
	data = op.read()
	data = ungzip(data)
	print(data.decode())
	


---
####  参考资料：  ####
- [http://www.zhihu.com/question/20899988](http://www.zhihu.com/question/20899988)
- [http://jecvay.com/2014/09/python3-web-bug-series2.html#more-310](http://jecvay.com/2014/09/python3-web-bug-series2.html#more-310)
- [http://blog.csdn.net/evankaka/article/details/46849095](http://blog.csdn.net/evankaka/article/details/46849095)
