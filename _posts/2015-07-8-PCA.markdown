---
layout: post
title:  "Mathematically, Principal Component Analysis"
date:   2015-11-18 20:21:21
categories: Tutorial
tags: PCA
permalink: /pca/
---

####Prologue
In this post, Principal Component Analysis will be broken down into mathematical bits one can easily understand. For a better insight a quick walkthrough of important mathematical concepts that you might have studied but haven't used in the real sense is done where necessary. The entire concept is explained by using an example, Hope this approach will help in complete understanding of the topic.


####Principal Component Analysis
It is the oldest and the most widely used statistical multivariate technique which finds a pattern in the data under consideration. PCA can be further extended towards dimensionality reduction by extracting important features which are necessary to the given problem statement and ignoring the ones which aren't. It find's its application in face recognition, image compression, dimensionality reduction etc.

Let us get into the nitty gritty, First, I will explain by taking an example and applying PCA to a simple one dimensional dataset. Further will take two dimensions and extend them to multi dimensions.

Consider a data set that you would like to study. As an example let us take some random **one dimensional** dataset _X_
		
	X = { 5,2,3,9,1,4 }

Now calculating the **mean** of _X_ with _n_ number of data by using the formula

$${\bar{X} = \frac{ \sum_ {i=1}^n X_i}{n}}$$

Thus, the mean, \\(\bar{X}\\) will now be equal to _4_. Mean as everybody knows it gives the center point of the dataset.

Now consider another dataset 

	Y = { 5,3,2,7,2,5 }

Notice the mean, \\(\bar{Y}\\) of this dataset is also _4_. So now how do you know how these two data are different? Or how they variy?

**Standard Deviation**. Wikipedia defines it as _"A measure that is used to quantify the amount of variation or dispersion of a set of data values"_ which simply means it gives a picture about how the data varies with respect to the mean. For the _X_ dataset standard deviation is given by the formula

$$SD = \sqrt{\frac{ \sum_ {i=1}^n {(X_i-\bar{X})^2}}{(n-1)}}$$

Took me sometime to understand as to why `(n-1)` is used. Wikipedia has the [perfect explanation](https://en.wikipedia.org/wiki/Standard_deviation). As of now, understand it is Bessel's correction and applying it would give a better answer.

Now **variance** is nothing but the square of standard deviation.

$$var = SD^2$$

or

$$var={\frac{ \sum_ {i=1}^n {(X_i-\bar{X})^2}}{(n-1)}}$$

The variance of _X_ is 8 and that of _Y_ is 4. Which tells us that the data of _X_ is more varied or spread across than the data of _Y_

Now let us consider two dimensional data, `(X,Y)` say

| X | _2_ | _3_ | _5_ | _7_ | _2_ | _1_ |
|---|:--:|--:| --:| --:| --:| --:|
| **Y** | _**1**_ | _**4**_ | _**8**_ | _**7**_ | _**1**_ | _**3**_ |

Repeat the same process as it was done for one dimensional data set by considering _X_ and _Y_ separate.
We will yield the following result,

Mean of X,		\\(\bar{X}\\) = 3.3335

Mean of Y,		\\(\bar{Y}\\) = 4.0

Variance of X,		\\(X_{var}\\) = 5.0667

Variance of Y,		\\(Y_{var}\\) = 8.8

Now we will introduce another parameter that will give the relation between the two dimensions. **Covariance**.

Covariance can be understood as variance but between two parameters and not by itself. Hence defined as

![](/assets/images/pca/2.png)

If you observe covariance and variance formula we can conclude that the covariance of itself that is cov(X,X) is nothing but the variance of X. ( If you did not understand, rewrite the above formula for cov(X,X) and you will see for yourself).

For the above considered example the covariance is, cov(X,Y) = 5.6

We can represent the same in a matrix. This matrix is also called as the variance-covariance matrix.

![](/assets/images/pca/1.png)

Note that cov(X,X) = \\(X_{var}\\)

and cov(Y,Y) = \\(Y_{var}\\)

also cov(X,Y) = cov(Y,X)

Thus now the variance-covariance matrix can be re-written as

![](/assets/images/pca/10.png)

Substituting the values as calculated before we get the variance-covariance matrix for the example dataset `(X,Y)` as

![](/assets/images/pca/3.png)


Now that we have the matrix to actually know which dataset has the most information and to extraxt the "Principal" information. We calculate the eigen values and eigen vectors

Note: In case you have forgotten how to calculate the eigen values and vectors refer [this document](https://www.scss.tcd.ie/Rozenn.Dahyot/CS1BA1/SolutionEigen.pdf).

For the example (X,Y)

eigen values are

![](/assets/images/pca/5.png)

and the eigen vectors are

![](/assets/images/pca/4.png)

The two eigen vectors obtained are perpendicular to one another.

Now comes the most interesting part. Read carefully, _"The eigen vector with the highest eigen value is the principal component of the data under consideration"_. This is what PCA is all about.

Here in this example the second eigen vector becomes the principal component as it has a higher eigen value of 12.83625 thus we write a feature matrix for the data set containing the eigen vectors in order of higest to lowest importance. Thus we get

![](/assets/images/pca/6.png)

Now that we know the principal component, The last step in PCA is to get the final transformed data.

Let us call the final dataset as _Findata_. The above feature matrix as _featuremat_

Consider the data set `(X,Y)`

| X | _2_ | _3_ | _5_ | _7_ | _2_ | _1_ |
|---|:--:|--:| --:| --:| --:| --:|
| **Y** | _**1**_ | _**4**_ | _**8**_ | _**7**_ | _**1**_ | _**3**_ |

To make the mean 0. Subtract every data in the respective datasets with its mean. (i.e. \\(X-\bar{X}\\) )

The dataset `(X,Y)` now becomes

| X | _-1.33333333_ | _-0.33333333_ |  _1.66666667_ | _3.66666667_ | _-1.33333333_ | _-2.33333333_ |
|---|:--:|--:| --:| --:| --:| --:|
| **Y** | _**-3**_ | _**0**_ | _**4**_ | _**3**_ | _**-3**_ | _**-1**_ |

Let us represent this in an array and call it _datam_

![](/assets/images/pca/7.png)

Let us call the final transformed dataset as _Findata_. The feature matrix as _featuremat_

Take the transpose of _feauturemat_ so that the highest eigen vector comes to the first row and the next highest to the second row and so on.

Now _featuremat_ becomes

![](/assets/images/pca/11.png)

We take the transpose of the _datam_ if the first column contains the first dataset and second the second dataset so when transpose is taken first row will have the first dataset, second row the second dataset and so on. But in this example we have considered the _datam_ itself having first row as first dataset, second row with second dataset. hence no need of transpose.

The very last step in PCA is

$$Findata = featuremat \times datam$$

Thus we get the final transformed data.

In this case _Findata_ is

![](/assets/images/pca/8.png)

This is final data obtained. To reduce a dimension that is to make this two dimensional data into one dimensional data all you have to do is consider only the highest eigen vector in the feature matrix and continue with the same steps and you will successfully reduce the data set into one dimension. The _Findata_ so obtained when only the highest eigen vector is considered is

![](/assets/images/pca/9.png)


This is what is otherwise called as data reduction or compression etc which will be discussed in detail in a separate post.

Now consider Multi dimensional data say `(A,B,C,....)`. Everything is same as that of two dimensional dataset. While considering eigenvectors. You can consider how many every you want based on how much reduction in data you want.

If you want to get the original data back from the transformed data you can read about recontruction part of PCA [here](http://www.rajathkumar.com/pca/reconstruction)

Now that you have understood PCA in further posts I will explain about implementing PCA in python, It's application in image compression and how it can be used for facial recognition with and without neural nets.

For any doubts feel free to comment or drop a mail to _rajathkumar.exe@gmail.com_ 

Peace.



 

